---
data-background: assets/nn.png
data-background-opacity: 0.05
---

## NN arguments

Actually PDFs are functions of 3 arguments:

<img src="assets/pdf-expr.png" alt="PDF expression">

So one should expect to see them as three inputs to the neural network, but it's
not the case.

@---

## $Q^2 \to$ evolution

As we saw in the _theory_ part, the PDF $Q^2 = \mu_F^2$ dependence is accounted
for by the DGLAP equations, so there is no reason of fitting it.

For this reason the fitting scale is fixed, and in order to compare with the
_experimental data_ the fitted candidate is evolved through <em style="color:
#f7ad00">FKtables</em>.

@---

## Flavor is in the _output_

Actually this dimension was addressed differently in the past w.r.t. the
present/future implementation:

- before there was one separate NN per fitted flavor
- at the moment the NN is asked for a vectorial output

<div style="display: flex; justify-content: center">
  <img src="assets/flav-output.png" alt="flavor in the output">
</div>

<p style="font-size: 0.6em; font-style: italic;">
Actually I didn't work on it, but I believe that in this way PDFs that shares
similar features are fitted by the same structure, factorizing the effort w.r.t.
the full duplication.
</p>

@---

## $\log x$ input

<style>
.reveal p { margin: 0.2em }
</style>

<p style="font-size: 0.7em; margin-bottom: 0.5em">
  Actually the PDF has two different regime:
</p>

<ul>
  <li>
    the large $x$, in which the natural variable is actually $x$
  </li>
  <li>
    the small $x$, in which instead the natural dependence dictated by the
    equations is on $\log x$
  </li>
</ul>

<p style="font-size: 0.7em; margin-bottom: 0.1em">
  For this reason two different inputs are given to the neural net, just to teach
  in advance some theoretical knowledge.
</p>
<p style="font-size: 0.4em; margin-bottom: 1.5em">
  Moreover the neural network would freeze for the small $x$, since actually the
  difference is small, but instead there is much difference if considered in
  $\log x$.
</p>

<div style="display: flex; justify-content: center">
  <img src="assets/xlogx.png" alt="nn x-logx input" style="margin-right: 2em">
  <div>
  <h5 style="font-size: 0.8em">feature scaling</h5>
  <p style="font-size: 0.5em">
  Another option that is being investigated <em>right now</em> is to scale the
  input according to the data distribution.
  </p>
  <p style="font-size: 0.5em">
  This is a common practice in the <em>ML community</em>, but it would be a
  novelty for PDFs.
  </p>
  </div>
</div>
