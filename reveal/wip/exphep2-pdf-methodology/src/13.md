## SGD

There are multiple alternatives related to **GD**:

<div style="display: flex">
  <div>
    <dl style="font-size: 0.7em">
      <dt><span style="color: #4976C5">classic</span></dt>
      <dd>in order to compute the real gradient you need to compute the loss on the
      <strong>full</strong> dataset, <em>it is expensive</em></dd>
      <dt><span style="color: #05B254">mini-batches</span></dt>
      <dd>you can decide to evaluate the loss only on a small subset per iteration,
      a <strong>mini-batch</strong>, assuming it is representative on average</dd>
      <dt><span style="color: #7132A1">stochastic</span></dt>
      <dd>limit of mini-batches of size $1$</dd>
    </dl>
  </div>
  <div>
    <img src="assets/sgd.png" alt="SGD and minibatch">
  </div>
</div>

In the last two cases the full dataset is explored in multiple steps, and
a full iteration is called an <strong>epoch</strong>.

@---

## Even more kinds of GD

<img src="assets/gd.gif" alt="Gradient Descent kinds comparison" height="450vh">
