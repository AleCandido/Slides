\documentclass[9pt]{beamer}

\geometry{paperwidth=213.3mm,paperheight=120mm}

\usetheme[{titleformat plain}=smallcaps,
           titleformat title=smallcaps,
           titleformat subtitle=regular,
           titleformat section=smallcaps,
           titleformat frame=smallcaps,
           % numbering=fraction,
          ]{metropolis}
% \usepackage{appendixnumberbeamer}

\definecolor{mLightGreen}{HTML}{14B03D}
\definecolor{vpGreen}{HTML}{66c2a5}
\definecolor{vpOrange}{HTML}{fc8d62}
\providecommand{\iRef}[1]{{\color{mLightGreen}\small $[$#1$]$}}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{../_style/common}
\usepackage{../_style/defs}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{amsmath, bm}
\usepackage{siunitx}
\usepackage{physics,hepnames}
\usepackage{mathtools}
\usepackage{enumitem}
\setenumerate[1]{%
      label=\protect\usebeamerfont{enumerate item}%
      \protect\usebeamercolor[fg]{enumerate item}%
      \insertenumlabel.}
\setitemize{label=\usebeamerfont*{itemize item}%
    \usebeamercolor[fg]{itemize item}
      \usebeamertemplate{itemize item}}

\usepackage{subfig}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{pifont}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{ulem}

\graphicspath{{pictures/}}

\title{PDF sampling}
\subtitle{as \sout{Bayesian} na\"ive as possible}
\date{August, 2022}
\author{\textit{\textbf{Alessandro Candido}}, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo}
%\institute{N3PDF}
\titlegraphic{
    \raisebox{10pt}[0pt][0pt]{\includegraphics[width=2.5cm]{../_logos/nnpdf_logo.pdf}}\hspace*{10pt}
    \hfill
    \raisebox{5pt}[0pt][0pt]{\includegraphics[height=0.8cm]{../_logos/n3pdf_logo.pdf}}\hspace*{10pt}
    \includegraphics[height=1.3cm]{../_logos/erc_logo1.png}

    \vfill\vspace*{230pt}
    \includegraphics[height=1cm]{../_logos/unimi_logo.png}\hfill
    \includegraphics[height=1cm]{../_logos/infn_logo.png}\\
    \vspace*{5pt}
    {
        \fontsize{3pt}{3.5pt}\selectfont
        \begin{center}
            This project has received funding from the European Union's Horizon
            2020 research and innovation programme under grant agreement No
            740006\quad \includegraphics[height=5pt]{../_logos/eu-flag.jpg}
        \end{center}
    }
}

\begin{document}

\maketitle

\setlist[description]{font=\quad\normalfont\bfseries\scshape\space}
\metroset{block=fill}

\begin{frame}{Function space}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            A function $f: \mathbb{R} \to \mathbb{R}$ (or suitable intervals)
            lives in an infinite-dimensional space.
            \vspace*{20pt}

            This has a simple consequence:
            \begin{block}{Under-determination}
                Fitting an \textbf{unknown function} on a finite number of data
                is always an \textbf{under-determined} problem.
            \end{block}
            \vspace*{20pt}

            How to choose a solution, when \alert{many} are available and
            \alert{equivalent}?
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=0.6\textwidth]{underdetermined}
                \caption{Possible $\chi^2$ profile in 2D parameter space.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Solutions in the Assumptions}
    \vspace*{10pt}
    \begin{center}
        There are two main ways to attack the problem.
    \end{center}
    \vspace*{10pt}

    \begin{columns}
        \begin{column}{0.5\textwidth}
            1. One consists in \textit{reducing the number of parameters}, by
            \textbf{slicing} a suitable \textbf{hyperplane}.
            \footnote{Not containing zero-directions.}
            \vspace*{10pt}

            This is what we do in PDFs when choosing a \alert{\textbf{fixed
            parametrization}}: we decide which parameters to fit, and take a
            single given value for everything else.
            \vspace*{10pt}

            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{sliced}
                \caption{$\chi^2$ profile in sliced parameter space.}
            \end{figure}
            \vspace*{10pt}
        \end{column}
        \begin{column}{0.5\textwidth}
            2. The second approach removes the zero-direction by adding
            \textbf{regularization}.
            \vspace*{10pt}

            This is what the \alert{\textbf{Neural Network}} (and its training
            algorithm) is doing under the hood.
            \vspace*{10pt}

            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{regularized}
                \caption{$\chi^2$ profile plus regularization function.}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{So Which?}
    Then which one should we choose?

    There is not an absolute best answer, because both procedures might be arbitrary.
    According to me, the principle to follow is "choose that convey more
    physical insight" in such a way to trade physical assumptions for arbitrariness.
\end{frame}

\begin{frame}{The NN}
    What is then doing the neural network (NN)? Why is it working so well?

    First: "working" is ambiguous, because we don't know the answer. Closure
    tests are a clue, but we want something to perform well on the truth, and
    not knowing it we are extrapolating from something that is working well on
    a truth candidate.

    Second: interpolation assumption in the training.
\end{frame}

\begin{frame}{Looking for Insights}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            NNPDF methodology has many good things:
            - nice representation of a generic distribution (replicas)
            - many tests in place
            - fast theory
            - lots of data

            We don't want to give up on any of these.
        \end{column}

        \begin{column}{0.5\textwidth}
            On the other hand, a pain point is the insight we have on the
            representation itself: we trust on it because of tests, but it is sometimes
            difficult to pin down results to features.

            \begin{exampleblock}{Hopscotch}
                There are some physical assumptions we can check:
                - integrability and sum rules
                - positivity
                - t0 chi2

                But the result might be more subtle: might be hidden in regularization.
            \end{exampleblock}

            Another limitation\footnote{Actually, this is where everything started
            from.} is that the current methodology need to make a fit to a sample at a
            time. The distribution is only the result of many fits.
            What if we could fit the distribution all at once?
            \vspace*{20pt}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Restart from Bayes}
    Typical examples of ML are image and speech recognition, generative tasks,
    style transfer and so on.

    All these problems have in common very high dimensional objects, with poor
    analytical insight on its structure. Working out an explicit and effective
    representation for them is difficult.

    This is not the case of PDF: we already describe them with a math language,
    and we have clear analytic properties at hand (sum rules, power-like
    behavior, and so on).
    
    ---

    Then, (Bayes).

    \begin{equation*}
        P(A|B) = \frac{P(B|A) P(A)}{P(B)}
    \end{equation*}
    
    Advantage: an explicit prior will do what the NN is now doing.
\end{frame}

\begin{frame}{Prior Choice \& Implementation}
    Gaussian process
    
    ---

    Fit Basic idea: take a bunch of points out of the function, the rest comes
    by interpolation. Which ones? LHAPDF's of course :)
\end{frame}


\begin{frame}{(Very) Preliminary Results}
    \vspace*{10pt}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{fit-pdf}
        \caption{
            Even with completely random data and theory, we get a valence like
            structure (\textbf{valence peak}, wrong place, wrong height) by
            virtue of \textbf{sum rules}. Still, they are too much
            unconstrained to get correct values.
        }
    \end{figure}
\end{frame}

\begin{frame}[standout]
    Questions?
\end{frame}

\appendix

\begin{frame}{PDF features}
    \begin{description}
        \item[sum rules] analytically, zero errors data point on the primitive
        \item[quadratic points] least square fit
        \item[PDF positivity] we can fit the log processes
    \end{description}
\end{frame}

\begin{frame}{Further results}
    \vspace*{30pt}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{fit-pdf-new}
                \caption{
                    New fit candidate, with less random data (still pretty
                    random).
                }
            \end{figure} 
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{figure}
                \centering
                \includegraphics[width=\textwidth]{fit-hyper-new}
                \caption{
                    Hyperparameters fit: notice that while exponents are
                    working, we are still missing the correlation length.
                }
            \end{figure} 
        \end{column}
    \end{columns}
\end{frame}

\end{document}
